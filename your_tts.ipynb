{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc05800-f373-42bf-aa79-cdc531944f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.bin.compute_embeddings import compute_embeddings\n",
    "from TTS.bin.resample import resample_files\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.vits_config import VitsConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig\n",
    "from TTS.utils.downloaders import download_vctk\n",
    "\n",
    "torch.set_num_threads(24)\n",
    "\n",
    "# pylint: disable=W0105\n",
    "\"\"\"\n",
    "    This recipe replicates the first experiment proposed in the YourTTS paper (https://arxiv.org/abs/2112.02418).\n",
    "    YourTTS model is based on the VITS model however it uses external speaker embeddings extracted from a pre-trained speaker encoder and has small architecture changes.\n",
    "    In addition, YourTTS can be trained in multilingual data, however, this recipe replicates the single language training using the VCTK dataset.\n",
    "    If you are interested in multilingual training, we have commented on parameters on the VitsArgs class instance that should be enabled for multilingual training.\n",
    "    In addition, you will need to add the extra datasets following the VCTK as an example.\n",
    "\"\"\"\n",
    "CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "# Name of the run for the Trainer\n",
    "RUN_NAME = \"YourTTS-EN-VCTK\"\n",
    "\n",
    "# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\n",
    "OUT_PATH = os.path.dirname(os.path.abspath(__file__))  # \"/raid/coqui/Checkpoints/original-YourTTS/\"\n",
    "\n",
    "# If you want to do transfer learning and speedup your training you can set here the path to the original YourTTS model\n",
    "RESTORE_PATH = None  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n",
    "\n",
    "# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\n",
    "SKIP_TRAIN_EPOCH = False\n",
    "\n",
    "# Set here the batch size to be used in training and evaluation\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n",
    "# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\n",
    "MAX_AUDIO_LEN_IN_SECONDS = 10\n",
    "\n",
    "### Download VCTK dataset\n",
    "VCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n",
    "# Define the number of threads used during the audio resampling\n",
    "NUM_RESAMPLE_THREADS = 10\n",
    "# Check if VCTK dataset is not already downloaded, if not download it\n",
    "if not os.path.exists(VCTK_DOWNLOAD_PATH):\n",
    "    print(\">>> Downloading VCTK dataset:\")\n",
    "    download_vctk(VCTK_DOWNLOAD_PATH)\n",
    "    resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)\n",
    "\n",
    "# init configs\n",
    "vctk_config = BaseDatasetConfig(\n",
    "    formatter=\"vctk\",\n",
    "    dataset_name=\"vctk\",\n",
    "    meta_file_train=\"\",\n",
    "    meta_file_val=\"\",\n",
    "    path=VCTK_DOWNLOAD_PATH,\n",
    "    language=\"en\",\n",
    "    ignored_speakers=[\n",
    "        \"p261\",\n",
    "        \"p225\",\n",
    "        \"p294\",\n",
    "        \"p347\",\n",
    "        \"p238\",\n",
    "        \"p234\",\n",
    "        \"p248\",\n",
    "        \"p335\",\n",
    "        \"p245\",\n",
    "        \"p326\",\n",
    "        \"p302\",\n",
    "    ],  # Ignore the test speakers to full replicate the paper experiment\n",
    ")\n",
    "\n",
    "# Add here all datasets configs, in our case we just want to train with the VCTK dataset then we need to add just VCTK. Note: If you want to add new datasets, just add them here and it will automatically compute the speaker embeddings (d-vectors) for this new dataset :)\n",
    "DATASETS_CONFIG_LIST = [vctk_config]\n",
    "\n",
    "### Extract speaker embeddings\n",
    "SPEAKER_ENCODER_CHECKPOINT_PATH = (\n",
    "    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n",
    ")\n",
    "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\n",
    "\n",
    "D_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n",
    "\n",
    "# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\n",
    "for dataset_conf in DATASETS_CONFIG_LIST:\n",
    "    # Check if the embeddings weren't already computed, if not compute it\n",
    "    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n",
    "    if not os.path.isfile(embeddings_file):\n",
    "        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n",
    "        compute_embeddings(\n",
    "            SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "            SPEAKER_ENCODER_CONFIG_PATH,\n",
    "            embeddings_file,\n",
    "            old_speakers_file=None,\n",
    "            config_dataset_path=None,\n",
    "            formatter_name=dataset_conf.formatter,\n",
    "            dataset_name=dataset_conf.dataset_name,\n",
    "            dataset_path=dataset_conf.path,\n",
    "            meta_file_train=dataset_conf.meta_file_train,\n",
    "            meta_file_val=dataset_conf.meta_file_val,\n",
    "            disable_cuda=False,\n",
    "            no_eval=False,\n",
    "        )\n",
    "    D_VECTOR_FILES.append(embeddings_file)\n",
    "\n",
    "\n",
    "# Audio config used in training.\n",
    "audio_config = VitsAudioConfig(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    hop_length=256,\n",
    "    win_length=1024,\n",
    "    fft_size=1024,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=None,\n",
    "    num_mels=80,\n",
    ")\n",
    "\n",
    "# Init VITSArgs setting the arguments that are needed for the YourTTS model\n",
    "model_args = VitsArgs(\n",
    "    d_vector_file=D_VECTOR_FILES,\n",
    "    use_d_vector_file=True,\n",
    "    d_vector_dim=512,\n",
    "    num_layers_text_encoder=10,\n",
    "    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n",
    "    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n",
    "    resblock_type_decoder=\"2\",  # In the paper, we accidentally trained the YourTTS using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n",
    "    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n",
    "    # use_speaker_encoder_as_loss=True,\n",
    "    # Useful parameters to enable multilingual training\n",
    "    # use_language_embedding=True,\n",
    "    # embedded_language_dim=4,\n",
    ")\n",
    "\n",
    "# General training config, here you can change the batch size and others useful parameters\n",
    "config = VitsConfig(\n",
    "    output_path=OUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=RUN_NAME,\n",
    "    project_name=\"YourTTS\",\n",
    "    run_description=\"\"\"\n",
    "            - Original YourTTS trained using VCTK dataset\n",
    "        \"\"\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,\n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=5000,\n",
    "    save_n_checkpoints=2,\n",
    "    save_checkpoints=True,\n",
    "    target_loss=\"loss_1\",\n",
    "    print_eval=False,\n",
    "    use_phonemes=False,\n",
    "    phonemizer=\"espeak\",\n",
    "    phoneme_language=\"en\",\n",
    "    compute_input_seq_cache=True,\n",
    "    add_blank=True,\n",
    "    text_cleaner=\"multilingual_cleaners\",\n",
    "    characters=CharactersConfig(\n",
    "        characters_class=\"TTS.tts.models.vits.VitsCharacters\",\n",
    "        pad=\"_\",\n",
    "        eos=\"&\",\n",
    "        bos=\"*\",\n",
    "        blank=None,\n",
    "        characters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\\u00af\\u00b7\\u00df\\u00e0\\u00e1\\u00e2\\u00e3\\u00e4\\u00e6\\u00e7\\u00e8\\u00e9\\u00ea\\u00eb\\u00ec\\u00ed\\u00ee\\u00ef\\u00f1\\u00f2\\u00f3\\u00f4\\u00f5\\u00f6\\u00f9\\u00fa\\u00fb\\u00fc\\u00ff\\u0101\\u0105\\u0107\\u0113\\u0119\\u011b\\u012b\\u0131\\u0142\\u0144\\u014d\\u0151\\u0153\\u015b\\u016b\\u0171\\u017a\\u017c\\u01ce\\u01d0\\u01d2\\u01d4\\u0430\\u0431\\u0432\\u0433\\u0434\\u0435\\u0436\\u0437\\u0438\\u0439\\u043a\\u043b\\u043c\\u043d\\u043e\\u043f\\u0440\\u0441\\u0442\\u0443\\u0444\\u0445\\u0446\\u0447\\u0448\\u0449\\u044a\\u044b\\u044c\\u044d\\u044e\\u044f\\u0451\\u0454\\u0456\\u0457\\u0491\\u2013!'(),-.:;? \",\n",
    "        punctuations=\"!'(),-.:;? \",\n",
    "        phonemes=\"\",\n",
    "        is_unique=True,\n",
    "        is_sorted=True,\n",
    "    ),\n",
    "    phoneme_cache_path=None,\n",
    "    precompute_num_workers=12,\n",
    "    start_by_longest=True,\n",
    "    datasets=DATASETS_CONFIG_LIST,\n",
    "    cudnn_benchmark=False,\n",
    "    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,\n",
    "    mixed_precision=False,\n",
    "    test_sentences=[\n",
    "        [\n",
    "            \"It took me quite a long time to develop a voice, and now that I have it I'm not going to be silent.\",\n",
    "            \"VCTK_p277\",\n",
    "            None,\n",
    "            \"en\",\n",
    "        ],\n",
    "        [\n",
    "            \"Be a voice, not an echo.\",\n",
    "            \"VCTK_p239\",\n",
    "            None,\n",
    "            \"en\",\n",
    "        ],\n",
    "        [\n",
    "            \"I'm sorry Dave. I'm afraid I can't do that.\",\n",
    "            \"VCTK_p258\",\n",
    "            None,\n",
    "            \"en\",\n",
    "        ],\n",
    "        [\n",
    "            \"This cake is great. It's so delicious and moist.\",\n",
    "            \"VCTK_p244\",\n",
    "            None,\n",
    "            \"en\",\n",
    "        ],\n",
    "        [\n",
    "            \"Prior to November 22, 1963.\",\n",
    "            \"VCTK_p305\",\n",
    "            None,\n",
    "            \"en\",\n",
    "        ],\n",
    "    ],\n",
    "    # Enable the weighted sampler\n",
    "    use_weighted_sampler=True,\n",
    "    # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has\n",
    "    weighted_sampler_attrs={\"speaker_name\": 1.0},\n",
    "    weighted_sampler_multipliers={},\n",
    "    # It defines the Speaker Consistency Loss (SCL) Î± to 9 like the paper\n",
    "    speaker_encoder_loss_alpha=9.0,\n",
    ")\n",
    "\n",
    "# Load all the datasets samples and split traning and evaluation sets\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config.datasets,\n",
    "    eval_split=True,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")\n",
    "\n",
    "# Init the model\n",
    "model = Vits.init_from_config(config)\n",
    "\n",
    "# Init the trainer and ðŸš€\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")\n",
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
