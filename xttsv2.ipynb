{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e9e570-24e1-4239-8162-47475e41b681",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0dea1a-b071-4966-a3b9-43afe219cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe75366-4b79-4574-a2ef-04d573160ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 08:22:02.777761: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 08:22:03.368127: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "from TTS.config import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156bf601-5971-49d9-81f4-b2c5b938fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"ljspeech\",\n",
    "    meta_file_train=\"Mixed_formatted.txt\",\n",
    "    # meta_file_train = \"No_Shouting_formatted.txt\",\n",
    "    path=os.path.join(OUT_PATH, \"data/\"),\n",
    "    language = \"en\"\n",
    ")\n",
    "# Define here the dataset that you want to use for the fine-tuning on.\n",
    "# config_dataset = BaseDatasetConfig(\n",
    "#     formatter=\"ljspeech\",\n",
    "#     dataset_name=\"ljspeech\",\n",
    "#     path=\"/raid/datasets/LJSpeech-1.1_24khz/\",\n",
    "#     meta_file_train=\"/raid/datasets/LJSpeech-1.1_24khz/metadata.csv\",\n",
    "#     language=\"en\",\n",
    "# )\n",
    "\n",
    "# Add here the configs of the datasets\n",
    "DATASETS_CONFIG_LIST = [config_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f46e491-967e-4976-8a10-1c2630adee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define audio config\n",
    "audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=22050)\n",
    "# training parameters config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6df6ece-5546-4f78-a848-9b6972bd1f78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the path where XTTS v1.1.1 files will be downloaded\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, \"xttsv2_checkpoint\", \"XTTS_v2.0_original_model_files/\")\n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "# DVAE files\n",
    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n",
    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n",
    "\n",
    "# Set the path to the downloaded files\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))\n",
    "\n",
    "# download DVAE files if needed\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "# Download XTTS v2.0 checkpoint if needed\n",
    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n",
    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n",
    "\n",
    "# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.\n",
    "TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file\n",
    "XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file\n",
    "\n",
    "# download XTTS v2.0 files if needed\n",
    "if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):\n",
    "    print(\" > Downloading XTTS v2.0 files!\")\n",
    "    ModelManager._download_model_files(\n",
    "        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f02965-cd37-4e27-af80-5996efec877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init args and config\n",
    "\n",
    "XTTS_CHECKPOINT = \"xttsv2_checkpoint/tyler1_xttsv2-February-27-2024_11+49AM-2b31060/best_model.pth\"\n",
    "\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=int(132300*2),  # 18 secs\n",
    "    min_conditioning_length=66150,  # 3 secs\n",
    "    debug_loading_failures=True,\n",
    "    max_wav_length=255995*3,  # ~33 seconds\n",
    "    max_text_length=700,\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune\n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    gpt_num_audio_tokens=1026,\n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    "    # gpt_max_text_tokens = 500,\n",
    "    # gpt_max_prompt_tokens = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1249d2-ba61-4372-bffb-b2f128dead68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sentences generations\n",
    "# SPEAKER_REFERENCE = [\n",
    "#     \"data/wavs/1. Tyler1 THE_WORST_JUNGLER_EVER 1.wav\"  # speaker reference to be used in training test sentences\n",
    "# ]\n",
    "\n",
    "SPEAKER_REFERENCE = [\"data/wavs/\" + wav for wav in os.listdir('data/wavs/') if \"wav\" in wav]\n",
    "\n",
    "LANGUAGE = config_dataset.language\n",
    "\n",
    "config = GPTTrainerConfig(\n",
    "    output_path=OUT_PATH + '/xttsv2_checkpoint',\n",
    "    model_args=model_args,\n",
    "    run_name=\"tyler1_xttsv2\",\n",
    "    project_name=\"tyler1\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    # logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=1,\n",
    "    # batch_group_size=48,\n",
    "    eval_batch_size=1,\n",
    "    num_loader_workers=2,\n",
    "    # eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=10000,\n",
    "    save_n_checkpoints=1,\n",
    "    save_checkpoints=True,\n",
    "    # target_loss=\"loss\",\n",
    "    print_eval=True,\n",
    "    # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=True, # for multi-gpu training please make it False\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=5e-06,  # learning rate\n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    # it was adjusted accordly for the new step scheme\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[\n",
    "        {\n",
    "            \"text\": \"So he starts off level one just doing this. Like oh he's gonna like bro he losing, doesn't hit a single thing. Look at it like- what the fuck. Like bro okay whatever. Watch this top dive bro. I solo made Vayne one HP, right? Just wait out and fucking ghost, you twat.\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Okay whatever you're auto-ing a ward sure it's fine. Bro what are you d- just wait you fucking freak. Where's he walking to by the way? What the fuck! It's not a win-trade this guy played as our Jarvan too. He's a one-trick like bro.\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Hey! Sup? It's me, Tyler1 ready for the pre-alpha. We're back baby!\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "    ],\n",
    "    # mixed_precision = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "698da81d-da4f-4500-8e45-a7a99a344cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Found 558 files in /home/sagemaker-user/voiceclone/data\n"
     ]
    }
   ],
   "source": [
    "def formatter(root_path, manifest_file, **kwargs):  # pylint: disable=unused-argument\n",
    "    \"\"\"Assumes each line as ```<filename>|<transcription>```\n",
    "    \"\"\"\n",
    "    txt_file = os.path.join(root_path, manifest_file)\n",
    "    items = []\n",
    "    speaker_name = \"Tyler1\"\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n",
    "        for line in ttf:\n",
    "            cols = line.split(\"|\")\n",
    "            wav_file = os.path.dirname(os.path.abspath('__file__')) + f\"/data/wavs/{cols[0]}.wav\"\n",
    "            text = cols[1]\n",
    "            # print(text)\n",
    "            items.append({\"text\":text, \"audio_file\":wav_file, \"speaker_name\":speaker_name, \"root_path\": root_path})\n",
    "    return items\n",
    "\n",
    "\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config_dataset,\n",
    "    # DATASETS_CONFIG_LIST,\n",
    "    eval_split=True,\n",
    "    # eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=0.1,\n",
    "    formatter = formatter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8dfbb1-2499-460d-ab56-8250696bc6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DVAE weights restored from: /home/sagemaker-user/voiceclone/xttsv2_checkpoint/XTTS_v2.0_original_model_files/dvae.pth\n"
     ]
    }
   ],
   "source": [
    "# init the model from config\n",
    "model = GPTTrainer.init_from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737adcb9-c5f1-4b24-8bc1-5649609a56f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n",
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Current device: 0\n",
      " | > Num. of GPUs: 1\n",
      " | > Num. of CPUs: 4\n",
      " | > Num. of Torch Threads: 1\n",
      " | > Torch seed: 1\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=/home/sagemaker-user/voiceclone/xttsv2_checkpoint/tyler1_xttsv2-February-28-2024_08+23AM-2b31060\n",
      "\n",
      " > Model has 518442047 parameters\n"
     ]
    }
   ],
   "source": [
    "# init the trainer and 🚀\n",
    "\n",
    "# model_path = \"xttsv2_checkpoint/tyler1_xttsv2-February-19-2024_05+30AM-2b31060/\"\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        # continue_path = model_path,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=True,\n",
    "        grad_accum_steps=256, # batch_size * grad_accum_steps >= 256,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2ed33-898b-4e1f-beea-18421ab87e5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/1000\u001b[0m\n",
      " --> /home/sagemaker-user/voiceclone/xttsv2_checkpoint/tyler1_xttsv2-February-28-2024_08+23AM-2b31060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Filtering invalid eval samples!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m > EVALUATION \u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Total eval samples after filtering: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m   --> STEP: 0\u001b[0m\n",
      "     | > loss_text_ce: 0.024301966652274132  (0.024301966652274132)\n",
      "     | > loss_mel_ce: 2.593413829803467  (2.593413829803467)\n",
      "     | > loss: 2.617715835571289  (2.617715835571289)\n",
      "\n",
      "\u001b[1m   --> STEP: 1\u001b[0m\n",
      "     | > loss_text_ce: 0.017928346991539  (0.017928346991539)\n",
      "     | > loss_mel_ce: 3.4081075191497803  (3.4081075191497803)\n",
      "     | > loss: 3.4260358810424805  (3.4260358810424805)\n",
      "\n",
      "\u001b[1m   --> STEP: 2\u001b[0m\n",
      "     | > loss_text_ce: 0.02084057591855526  (0.01938446145504713)\n",
      "     | > loss_mel_ce: 3.547140598297119  (3.4776240587234497)\n",
      "     | > loss: 3.567981243133545  (3.4970085620880127)\n",
      "\n",
      "\u001b[1m   --> STEP: 3\u001b[0m\n",
      "     | > loss_text_ce: 0.02912505529820919  (0.022631326069434483)\n",
      "     | > loss_mel_ce: 3.1654651165008545  (3.3735710779825845)\n",
      "     | > loss: 3.1945900917053223  (3.3962024052937827)\n",
      "\n",
      "\u001b[1m   --> STEP: 4\u001b[0m\n",
      "     | > loss_text_ce: 0.02102237194776535  (0.0222290875390172)\n",
      "     | > loss_mel_ce: 3.2765908241271973  (3.349326014518738)\n",
      "     | > loss: 3.2976131439208984  (3.3715550899505615)\n",
      "\n",
      "\u001b[1m   --> STEP: 5\u001b[0m\n",
      "     | > loss_text_ce: 0.022306859493255615  (0.022244641929864882)\n",
      "     | > loss_mel_ce: 3.23685622215271  (3.3268320560455322)\n",
      "     | > loss: 3.2591631412506104  (3.3490767002105715)\n",
      "\n",
      "\u001b[1m   --> STEP: 6\u001b[0m\n",
      "     | > loss_text_ce: 0.021993350237607956  (0.022202759981155396)\n",
      "     | > loss_mel_ce: 4.208634853363037  (3.473799188931783)\n",
      "     | > loss: 4.23062801361084  (3.496001919110616)\n",
      "\n",
      "\u001b[1m   --> STEP: 7\u001b[0m\n",
      "     | > loss_text_ce: 0.02470349706709385  (0.022560008136289462)\n",
      "     | > loss_mel_ce: 2.980685234069824  (3.4033543382372176)\n",
      "     | > loss: 3.0053887367248535  (3.425914321626936)\n",
      "\n",
      "\u001b[1m   --> STEP: 8\u001b[0m\n",
      "     | > loss_text_ce: 0.02594146691262722  (0.02298269048333168)\n",
      "     | > loss_mel_ce: 2.7858588695526123  (3.326167404651642)\n",
      "     | > loss: 2.811800241470337  (3.349150061607361)\n",
      "\n",
      "\u001b[1m   --> STEP: 9\u001b[0m\n",
      "     | > loss_text_ce: 0.02669745311141014  (0.02339544188645151)\n",
      "     | > loss_mel_ce: 3.4128921031951904  (3.3358034822675915)\n",
      "     | > loss: 3.439589500427246  (3.359198888142904)\n",
      "\n",
      "\u001b[1m   --> STEP: 10\u001b[0m\n",
      "     | > loss_text_ce: 0.0238117054104805  (0.023437068238854407)\n",
      "     | > loss_mel_ce: 3.2874958515167236  (3.3309727191925047)\n",
      "     | > loss: 3.311307668685913  (3.3544097661972048)\n",
      "\n",
      "\u001b[1m   --> STEP: 11\u001b[0m\n",
      "     | > loss_text_ce: 0.025092653930187225  (0.023587576028975574)\n",
      "     | > loss_mel_ce: 3.3071863651275635  (3.3288103233684194)\n",
      "     | > loss: 3.3322789669036865  (3.3523978753523394)\n",
      "\n",
      "\u001b[1m   --> STEP: 12\u001b[0m\n",
      "     | > loss_text_ce: 0.021977955475449562  (0.023453440982848406)\n",
      "     | > loss_mel_ce: 3.3119547367095947  (3.3274056911468506)\n",
      "     | > loss: 3.333932638168335  (3.3508591055870056)\n",
      "\n",
      "\u001b[1m   --> STEP: 13\u001b[0m\n",
      "     | > loss_text_ce: 0.02531576342880726  (0.02359669655561447)\n",
      "     | > loss_mel_ce: 2.8400533199310303  (3.289917047207172)\n",
      "     | > loss: 2.8653690814971924  (3.3135137191185584)\n",
      "\n",
      "\u001b[1m   --> STEP: 14\u001b[0m\n",
      "     | > loss_text_ce: 0.020762788131833076  (0.023394274525344372)\n",
      "     | > loss_mel_ce: 2.690633535385132  (3.2471110820770264)\n",
      "     | > loss: 2.7113962173461914  (3.270505326134818)\n",
      "\n",
      "\u001b[1m   --> STEP: 15\u001b[0m\n",
      "     | > loss_text_ce: 0.02166549116373062  (0.02327902230123679)\n",
      "     | > loss_mel_ce: 3.489114999771118  (3.2632446765899656)\n",
      "     | > loss: 3.5107805728912354  (3.286523675918579)\n",
      "\n",
      "\u001b[1m   --> STEP: 16\u001b[0m\n",
      "     | > loss_text_ce: 0.025784112513065338  (0.023435590439476073)\n",
      "     | > loss_mel_ce: 3.520089626312256  (3.279297485947609)\n",
      "     | > loss: 3.5458736419677734  (3.3027330487966537)\n",
      "\n",
      "\u001b[1m   --> STEP: 17\u001b[0m\n",
      "     | > loss_text_ce: 0.021674739196896553  (0.023332010954618454)\n",
      "     | > loss_mel_ce: 3.534147024154663  (3.2942886352539062)\n",
      "     | > loss: 3.555821657180786  (3.3176206139957203)\n",
      "\n",
      "\u001b[1m   --> STEP: 18\u001b[0m\n",
      "     | > loss_text_ce: 0.019979605451226234  (0.02314576620443)\n",
      "     | > loss_mel_ce: 3.917370080947876  (3.3289042711257935)\n",
      "     | > loss: 3.937349796295166  (3.3520500130123563)\n",
      "\n",
      "\u001b[1m   --> STEP: 19\u001b[0m\n",
      "     | > loss_text_ce: 0.02539628930389881  (0.023264214788612566)\n",
      "     | > loss_mel_ce: 2.931793212890625  (3.3080036891134164)\n",
      "     | > loss: 2.9571895599365234  (3.331267883903102)\n",
      "\n",
      "\u001b[1m   --> STEP: 20\u001b[0m\n",
      "     | > loss_text_ce: 0.018620185554027557  (0.023032013326883316)\n",
      "     | > loss_mel_ce: 3.361403703689575  (3.310673689842224)\n",
      "     | > loss: 3.380023956298828  (3.333705687522888)\n",
      "\n",
      "\u001b[1m   --> STEP: 21\u001b[0m\n",
      "     | > loss_text_ce: 0.024177948012948036  (0.023086581645267352)\n",
      "     | > loss_mel_ce: 3.205117702484131  (3.3056472142537436)\n",
      "     | > loss: 3.2292957305908203  (3.3287337848118375)\n",
      "\n",
      "\u001b[1m   --> STEP: 22\u001b[0m\n",
      "     | > loss_text_ce: 0.019839558750391006  (0.022938989695500244)\n",
      "     | > loss_mel_ce: 3.060742139816284  (3.2945151654156772)\n",
      "     | > loss: 3.0805816650390625  (3.317454143003984)\n",
      "\n",
      "\u001b[1m   --> STEP: 23\u001b[0m\n",
      "     | > loss_text_ce: 0.024208368733525276  (0.022994180088457855)\n",
      "     | > loss_mel_ce: 3.306533098220825  (3.2950376842332925)\n",
      "     | > loss: 3.3307414054870605  (3.3180318500684653)\n",
      "\n",
      "\u001b[1m   --> STEP: 24\u001b[0m\n",
      "     | > loss_text_ce: 0.024865567684173584  (0.023072154571612675)\n",
      "     | > loss_mel_ce: 2.9308578968048096  (3.2798635264237723)\n",
      "     | > loss: 2.955723524093628  (3.302935669819514)\n",
      "\n",
      "\u001b[1m   --> STEP: 25\u001b[0m\n",
      "     | > loss_text_ce: 0.018542088568210602  (0.022890951931476593)\n",
      "     | > loss_mel_ce: 2.796692371368408  (3.2605366802215574)\n",
      "     | > loss: 2.815234422683716  (3.283427619934082)\n",
      "\n",
      "\u001b[1m   --> STEP: 26\u001b[0m\n",
      "     | > loss_text_ce: 0.02475661039352417  (0.02296270802617073)\n",
      "     | > loss_mel_ce: 3.5210139751434326  (3.2705550377185526)\n",
      "     | > loss: 3.5457706451416016  (3.2935177362882175)\n",
      "\n",
      "\u001b[1m   --> STEP: 27\u001b[0m\n",
      "     | > loss_text_ce: 0.02251807413995266  (0.02294624010445895)\n",
      "     | > loss_mel_ce: 3.260469913482666  (3.270181514598705)\n",
      "     | > loss: 3.2829880714416504  (3.2931277487013073)\n",
      "\n",
      "\u001b[1m   --> STEP: 28\u001b[0m\n",
      "     | > loss_text_ce: 0.023641051724553108  (0.0229710548051766)\n",
      "     | > loss_mel_ce: 3.147876739501953  (3.265813486916678)\n",
      "     | > loss: 3.171517848968506  (3.288784537996565)\n",
      "\n",
      "\u001b[1m   --> STEP: 29\u001b[0m\n",
      "     | > loss_text_ce: 0.022861244156956673  (0.022967268231100048)\n",
      "     | > loss_mel_ce: 3.141735076904297  (3.2615349210541824)\n",
      "     | > loss: 3.1645963191986084  (3.284502185624221)\n",
      "\n",
      "\u001b[1m   --> STEP: 30\u001b[0m\n",
      "     | > loss_text_ce: 0.02583683840930462  (0.023062920570373534)\n",
      "     | > loss_mel_ce: 3.8179638385772705  (3.2800825516382854)\n",
      "     | > loss: 3.8438007831573486  (3.303145472208659)\n",
      "\n",
      "\u001b[1m   --> STEP: 31\u001b[0m\n",
      "     | > loss_text_ce: 0.020538324490189552  (0.022981481987141793)\n",
      "     | > loss_mel_ce: 3.1902835369110107  (3.277185809227728)\n",
      "     | > loss: 3.2108218669891357  (3.300167291395126)\n",
      "\n",
      "\u001b[1m   --> STEP: 32\u001b[0m\n",
      "     | > loss_text_ce: 0.027006562799215317  (0.02310726576251909)\n",
      "     | > loss_mel_ce: 2.768183946609497  (3.2612795010209084)\n",
      "     | > loss: 2.7951905727386475  (3.284386768937111)\n",
      "\n",
      "\u001b[1m   --> STEP: 33\u001b[0m\n",
      "     | > loss_text_ce: 0.022039860486984253  (0.023074920148108944)\n",
      "     | > loss_mel_ce: 3.4340426921844482  (3.2665147492379853)\n",
      "     | > loss: 3.456082582473755  (3.2895896723776152)\n",
      "\n",
      "\u001b[1m   --> STEP: 34\u001b[0m\n",
      "     | > loss_text_ce: 0.02580185793340206  (0.023155124200617567)\n",
      "     | > loss_mel_ce: 3.093029260635376  (3.2614122348673202)\n",
      "     | > loss: 3.118831157684326  (3.2845673631219303)\n",
      "\n",
      "\u001b[1m   --> STEP: 35\u001b[0m\n",
      "     | > loss_text_ce: 0.018424605950713158  (0.023019966536334584)\n",
      "     | > loss_mel_ce: 3.390613555908203  (3.265103701182774)\n",
      "     | > loss: 3.4090380668640137  (3.2881236689431326)\n",
      "\n",
      "\u001b[1m   --> STEP: 36\u001b[0m\n",
      "     | > loss_text_ce: 0.02008511684834957  (0.022938442933890555)\n",
      "     | > loss_mel_ce: 3.5673208236694336  (3.273498621251848)\n",
      "     | > loss: 3.5874059200286865  (3.29643706480662)\n",
      "\n",
      "\u001b[1m   --> STEP: 37\u001b[0m\n",
      "     | > loss_text_ce: 0.023973500356078148  (0.022966417458814545)\n",
      "     | > loss_mel_ce: 3.221895217895508  (3.2721039346746497)\n",
      "     | > loss: 3.245868682861328  (3.2950703517810718)\n",
      "\n",
      "\u001b[1m   --> STEP: 38\u001b[0m\n",
      "     | > loss_text_ce: 0.020891934633255005  (0.022911825805510346)\n",
      "     | > loss_mel_ce: 3.6720001697540283  (3.2826275198083175)\n",
      "     | > loss: 3.692892074584961  (3.305539344486437)\n",
      "\n",
      "\u001b[1m   --> STEP: 39\u001b[0m\n",
      "     | > loss_text_ce: 0.02197364903986454  (0.022887769991006605)\n",
      "     | > loss_mel_ce: 3.9077932834625244  (3.2986574111840663)\n",
      "     | > loss: 3.929766893386841  (3.321545179073627)\n",
      "\n",
      "\u001b[1m   --> STEP: 40\u001b[0m\n",
      "     | > loss_text_ce: 0.022119905799627304  (0.022868573386222123)\n",
      "     | > loss_mel_ce: 3.138279676437378  (3.2946479678153993)\n",
      "     | > loss: 3.1603996753692627  (3.317516541481018)\n",
      "\n",
      "\u001b[1m   --> STEP: 41\u001b[0m\n",
      "     | > loss_text_ce: 0.022724943235516548  (0.02286507021181467)\n",
      "     | > loss_mel_ce: 3.261359214782715  (3.293836047009724)\n",
      "     | > loss: 3.2840840816497803  (3.3167011156314756)\n",
      "\n",
      "\u001b[1m   --> STEP: 42\u001b[0m\n",
      "     | > loss_text_ce: 0.021766453981399536  (0.022838912682519072)\n",
      "     | > loss_mel_ce: 3.25439190864563  (3.292896900858198)\n",
      "     | > loss: 3.276158332824707  (3.3157358112789335)\n",
      "\n",
      "\u001b[1m   --> STEP: 43\u001b[0m\n",
      "     | > loss_text_ce: 0.022718515247106552  (0.02283611274216064)\n",
      "     | > loss_mel_ce: 3.8236987590789795  (3.305241130119146)\n",
      "     | > loss: 3.846417188644409  (3.328077238659526)\n",
      "\n",
      "\u001b[1m   --> STEP: 44\u001b[0m\n",
      "     | > loss_text_ce: 0.020672950893640518  (0.022786949972876093)\n",
      "     | > loss_mel_ce: 3.3240675926208496  (3.305669004266912)\n",
      "     | > loss: 3.344740629196167  (3.3284559520808132)\n",
      "\n",
      "\u001b[1m   --> STEP: 45\u001b[0m\n",
      "     | > loss_text_ce: 0.024271590635180473  (0.022819941987593968)\n",
      "     | > loss_mel_ce: 2.7537729740142822  (3.293404648039076)\n",
      "     | > loss: 2.7780444622039795  (3.3162245856391057)\n",
      "\n",
      "\u001b[1m   --> STEP: 46\u001b[0m\n",
      "     | > loss_text_ce: 0.033638954162597656  (0.023055137904441875)\n",
      "     | > loss_mel_ce: 3.445357322692871  (3.296707967053289)\n",
      "     | > loss: 3.4789962768554688  (3.3197631006655484)\n",
      "\n",
      "\u001b[1m   --> STEP: 47\u001b[0m\n",
      "     | > loss_text_ce: 0.020882701501250267  (0.02300891585331014)\n",
      "     | > loss_mel_ce: 2.9655725955963135  (3.289662533618034)\n",
      "     | > loss: 2.986455202102661  (3.3126714432493167)\n",
      "\n",
      "\u001b[1m   --> STEP: 48\u001b[0m\n",
      "     | > loss_text_ce: 0.025422831997275352  (0.02305920577297608)\n",
      "     | > loss_mel_ce: 3.947585105895996  (3.303369253873825)\n",
      "     | > loss: 3.973007917404175  (3.326428453127543)\n",
      "\n",
      "\u001b[1m   --> STEP: 49\u001b[0m\n",
      "     | > loss_text_ce: 0.021127531304955482  (0.023019783845057293)\n",
      "     | > loss_mel_ce: 3.557924270629883  (3.3085642542157854)\n",
      "     | > loss: 3.5790517330169678  (3.3315840302681434)\n",
      "\n",
      "\u001b[1m   --> STEP: 50\u001b[0m\n",
      "     | > loss_text_ce: 0.02191721461713314  (0.02299773246049881)\n",
      "     | > loss_mel_ce: 2.879239082336426  (3.2999777507781984)\n",
      "     | > loss: 2.901156187057495  (3.3229754734039307)\n",
      "\n",
      "\u001b[1m   --> STEP: 51\u001b[0m\n",
      "     | > loss_text_ce: 0.02153000794351101  (0.02296895354840101)\n",
      "     | > loss_mel_ce: 2.628199577331543  (3.2868056297302246)\n",
      "     | > loss: 2.6497294902801514  (3.3097745717740525)\n",
      "\n",
      "\u001b[1m   --> STEP: 52\u001b[0m\n",
      "     | > loss_text_ce: 0.024354612454771996  (0.02299560083506199)\n",
      "     | > loss_mel_ce: 3.0546188354492188  (3.2823404990709744)\n",
      "     | > loss: 3.0789735317230225  (3.3053360902346096)\n",
      "\n",
      "\u001b[1m   --> STEP: 53\u001b[0m\n",
      "     | > loss_text_ce: 0.021922830492258072  (0.022975359885197767)\n",
      "     | > loss_mel_ce: 2.7471396923065186  (3.2722423706414565)\n",
      "     | > loss: 2.7690625190734863  (3.2952177209674187)\n",
      "\n",
      "\u001b[1m   --> STEP: 54\u001b[0m\n",
      "     | > loss_text_ce: 0.021296339109539986  (0.02294426690787077)\n",
      "     | > loss_mel_ce: 3.23549747467041  (3.2715619096049555)\n",
      "     | > loss: 3.256793737411499  (3.294506165716383)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " | > Synthesizing test sentences.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94610e-09a9-4db2-89cd-924dcc1e2f22",
   "metadata": {},
   "source": [
    "##  Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21649c2a-0d88-446d-a6c4-c93abe354d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "from subprocess import getoutput\n",
    "from IPython.display import Audio\n",
    "\n",
    "# using the default version set in 🐸TTS\n",
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)\n",
    "\n",
    "# using a specific version\n",
    "# 👀 see the branch names for versions on https://huggingface.co/coqui/XTTS-v2/tree/main\n",
    "# ❗some versions might be incompatible with the API\n",
    "# tts = TTS(\"xtts_v2.0.2\", gpu=True)\n",
    "\n",
    "# getting the latest XTTS_v2\n",
    "tts = TTS(\"xtts\").to(\"cuda\")\n",
    "\n",
    "SPEAKER_REFERENCE = [\"data/wavs/\" + wav for wav in os.listdir('data/wavs/') if \"wav\" in wav]\n",
    "\n",
    "tts.tts_to_file(text= \"What's up? What's up?! It's Tyler1 baby! Back with the voice\",\n",
    "                file_path=\"generated_audio/xtts_output.wav\",\n",
    "                speaker_wav=SPEAKER_REFERENCE,\n",
    "                language=\"en\"\n",
    "               )\n",
    "\n",
    "Audio(\"generated_audio/xtts_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b1097-dbba-417d-aec3-ac89ae2afe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import Xtts\n",
    "from subprocess import getoutput\n",
    "from IPython.display import Audio\n",
    "\n",
    "\n",
    "# Add here the xtts_config path\n",
    "CONFIG_PATH = \"xttsv2_checkpoint/tyler1_xttsv2-February-27-2024_11+49AM-2b31060/config.json\"\n",
    "# Add here the vocab file that you have used to train the model\n",
    "TOKENIZER_PATH = \"xttsv2_checkpoint/XTTS_v2.0_original_model_files/vocab.json\"\n",
    "# Add here the checkpoint that you want to do inference with\n",
    "XTTS_CHECKPOINT = \"xttsv2_checkpoint/tyler1_xttsv2-February-27-2024_11+49AM-2b31060/best_model.pth\"\n",
    "\n",
    "# List of all wavs for speaker reference\n",
    "wavs = getoutput(\"ls data/wavs/*.wav\").split(\"\\n\")\n",
    "# Add here the speaker reference\n",
    "SPEAKER_REFERENCE = [\"data/wavs/\" + wav for wav in os.listdir('data/wavs/') if \"wav\" in wav]\n",
    "\n",
    "# output wav path\n",
    "OUTPUT_WAV_PATH = \"generated_audio/xtts_output.wav\"\n",
    "\n",
    "\n",
    "config = XttsConfig()\n",
    "config.load_json(CONFIG_PATH)\n",
    "model = Xtts.init_from_config(config)\n",
    "model.load_checkpoint(config, checkpoint_path=XTTS_CHECKPOINT, vocab_path=TOKENIZER_PATH, use_deepspeed=False)\n",
    "model.cuda()\n",
    "\n",
    "gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(audio_path= SPEAKER_REFERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b424f-41cd-47cf-a204-cdd152b8084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "out = model.inference(\n",
    "    \"If it isn't my boy David! The one handed bandit himself!!\",\n",
    "    \"en\",\n",
    "    gpt_cond_latent,\n",
    "    speaker_embedding,\n",
    "    temperature=0.7, # Add custom parameters here\n",
    ")\n",
    "torchaudio.save(OUTPUT_WAV_PATH, torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "Audio(OUTPUT_WAV_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
